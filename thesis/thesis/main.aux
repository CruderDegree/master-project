\relax 
\providecommand*{\memsetcounter}[2]{}
\catcode `"\active 
\select@language{danish}
\@writefile{toc}{\select@language{danish}}
\@writefile{lof}{\select@language{danish}}
\@writefile{lot}{\select@language{danish}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{danish}
\@writefile{toc}{\select@language{danish}}
\@writefile{lof}{\select@language{danish}}
\@writefile{lot}{\select@language{danish}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{Dumontier2017}
\citation{Rojas96}
\citation{WikiCNN}
\citation{WikiCNN}
\citation{Wiki_RF}
\citation{Wiki_RF}
\citation{AlphaGo}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Convolutional Neural Networks}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Reinforcement Learning}{1}}
\citation{Hammer}
\citation{Hammer}
\citation{Hammer}
\citation{henrik_DLA}
\citation{henrik_DLA}
\citation{henrik_DLA}
\citation{Hammer}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}ASLA}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Building phase}{2}}
\citation{henrik_DLA}
\citation{henrik_DLA}
\citation{henrik_DLA}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Overview of the three phases in ASLA. For each step $t$ in the building phase of episode $i$, the current grid $s_t^{(i)}$ is inputted into the CNN, which outputs the Q-values of each gridpoint. Depending on the current build policy, an action $a_t^{(i)}$ is taken, placing an atom on the chosen grid point. This step is repeated until all atoms are placed. The completed structure $s_{final}^{(i)}$ is then passed onto the evaluation phase, where the property $\mathcal  {P}^{(i)}$ is calculated (e.g. a DFT energy calculation). The results from the evaluation phase are then saved in the Replay Buffer, storing state-action pairs and their associated property. During the training phase, a number of state-action pairs are loaded from the replay buffer and used to train the CNN using backpropagation to improve future Q-value estimates. The illustration is taken from \cite  {Hammer}.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ASLA_overview}{{\M@TitleReference {2.1}{Overview of the three phases in ASLA. For each step $t$ in the building phase of episode $i$, the current grid $s_t^{(i)}$ is inputted into the CNN, which outputs the Q-values of each gridpoint. Depending on the current build policy, an action $a_t^{(i)}$ is taken, placing an atom on the chosen grid point. This step is repeated until all atoms are placed. The completed structure $s_{final}^{(i)}$ is then passed onto the evaluation phase, where the property $\mathcal  {P}^{(i)}$ is calculated (e.g. a DFT energy calculation). The results from the evaluation phase are then saved in the Replay Buffer, storing state-action pairs and their associated property. During the training phase, a number of state-action pairs are loaded from the replay buffer and used to train the CNN using backpropagation to improve future Q-value estimates. The illustration is taken from \cite  {Hammer}.\relax }}{3}}
\citation{Hammer}
\citation{henrik_DLA}
\citation{henrik_DLA}
\citation{henrik_DLA}
\citation{henrik_DLA}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Evaluation Phase}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Replay buffer}{4}}
\citation{Hammer}
\citation{Hammer}
\citation{Meldgaard}
\citation{Meldgaard}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Training phase}{5}}
\newlabel{eq:update_rule}{{\M@TitleReference {2.1}{Training phase}}{5}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Bundled atomic energies using clustering}{5}}
\citation{Meldgaard}
\citation{Meldgaard}
\citation{Meldgaard}
\citation{Meldgaard}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Structure representation}{6}}
\newlabel{eq:global_feature_vector}{{\M@TitleReference {3.2}{Structure representation}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Local energies}{6}}
\newlabel{eq:Local_energies}{{\M@TitleReference {3.3}{Local energies}}{6}}
\citation{Meldgaard}
\citation{Meldgaard}
\newlabel{eq:Energy_bundles_matrix}{{\M@TitleReference {3.5}{Local energies}}{7}}
\newlabel{eq:Matrix_problem_rewritten}{{\M@TitleReference {3.6}{Local energies}}{7}}
\newlabel{eq:Matrix_problem_minimize}{{\M@TitleReference {3.7}{Local energies}}{7}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Motivation}{8}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Clustering by atomic bonds}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Structure representation}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Visualization of how structures are represented in a feature matrix. For illustration purposes the structures only consist of $C_2H_2$. Each entry in the matrix corresponds to the number of atoms with some specific bonds. As an example with the top structure shown, both $H$-atoms have $1$ $C$ bond an $0$ $H$ bonds, which is shown in the matrix at row 2 column 1. After examining all atoms in the structure, the matrix is flattened into a vector to estimate the local energy of each cluster. The structures can then be colored by the total number of bonds of each atom.\relax }}{10}}
\newlabel{fig:struc_representation}{{\M@TitleReference {5.1}{Visualization of how structures are represented in a feature matrix. For illustration purposes the structures only consist of $C_2H_2$. Each entry in the matrix corresponds to the number of atoms with some specific bonds. As an example with the top structure shown, both $H$-atoms have $1$ $C$ bond an $0$ $H$ bonds, which is shown in the matrix at row 2 column 1. After examining all atoms in the structure, the matrix is flattened into a vector to estimate the local energy of each cluster. The structures can then be colored by the total number of bonds of each atom.\relax }}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Demonstration of local features for each atom of a completed structure. \textbf  {Frame 1:} The red $H$-atom is too far away from the rest of the structure and is considered isolated by the feature function $g$. \textbf  {Frame 2:} Moving the $H$-atom to the right will bring it close enough to the remaining structure for $g$ to consider it a bond. \textbf  {Frame 3-4:} Further moving the atom to the right will also change the representation (color) of the other atoms as they become bonded or bonds break with this atom. Note in frame 3 that this representation only has a maximum distance and so it doesn't penalize placing atoms too close to each other.\relax }}{11}}
\newlabel{fig:bonding_demo}{{\M@TitleReference {5.2}{Demonstration of local features for each atom of a completed structure. \textbf  {Frame 1:} The red $H$-atom is too far away from the rest of the structure and is considered isolated by the feature function $g$. \textbf  {Frame 2:} Moving the $H$-atom to the right will bring it close enough to the remaining structure for $g$ to consider it a bond. \textbf  {Frame 3-4:} Further moving the atom to the right will also change the representation (color) of the other atoms as they become bonded or bonds break with this atom. Note in frame 3 that this representation only has a maximum distance and so it doesn't penalize placing atoms too close to each other.\relax }}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Results}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Demonstration of how estimating the potential energy of structures built by ASLA can be done using local energies dependent on the number and type of bonds of each atom in the structure. The $500$ blue points come from a random sample of builds by an agent that found the global energy minimum of $\SI {-70.17}{\electronvolt }$. These structures have all had their potential energy estimated by a DFT calculation, which is their position on the first axis. Their position on the second axis is determined by their estimated energy. The red line indicates the points where the DFT energy and the estimate are equal.\relax }}{12}}
\newlabel{fig:energy_prediction}{{\M@TitleReference {5.3}{Demonstration of how estimating the potential energy of structures built by ASLA can be done using local energies dependent on the number and type of bonds of each atom in the structure. The $500$ blue points come from a random sample of builds by an agent that found the global energy minimum of $\SI {-70.17}{\electronvolt }$. These structures have all had their potential energy estimated by a DFT calculation, which is their position on the first axis. Their position on the second axis is determined by their estimated energy. The red line indicates the points where the DFT energy and the estimate are equal.\relax }}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Demonstration of how the proposed filtering algorithm works. The estimated energy of each cluster is calculated from $500$ structures. \textbf  {a:} The agent builds $N$ structures that are saved in a list. In this example the DFT calculation has been done to show the true target property of each structure. \textbf  {b:} A feature vector is then calculated for each structure and using this, the energy of the complete structure is estimated. \textbf  {c:} The filtering then consists of choosing the $N/2$ structures with the lowest estimated energy.\relax }}{13}}
\newlabel{fig:filtering}{{\M@TitleReference {5.4}{Demonstration of how the proposed filtering algorithm works. The estimated energy of each cluster is calculated from $500$ structures. \textbf  {a:} The agent builds $N$ structures that are saved in a list. In this example the DFT calculation has been done to show the true target property of each structure. \textbf  {b:} A feature vector is then calculated for each structure and using this, the energy of the complete structure is estimated. \textbf  {c:} The filtering then consists of choosing the $N/2$ structures with the lowest estimated energy.\relax }}{13}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Implementation into ASLA}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Hyperparameter search}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The chance to make a completely random action during $\epsilon $-policy builds, $\epsilon _0$, was varied between $1/10$ through $7/10$. After initiating 25 agents with each value of $\epsilon _0$, the number of agents that found the global minimum solution were counted. The figure shows that the setting $\epsilon _0 = 4/10$ proved the most fruitful.\relax }}{15}}
\newlabel{fig:epsilon_variation}{{\M@TitleReference {6.1}{The chance to make a completely random action during $\epsilon $-policy builds, $\epsilon _0$, was varied between $1/10$ through $7/10$. After initiating 25 agents with each value of $\epsilon _0$, the number of agents that found the global minimum solution were counted. The figure shows that the setting $\epsilon _0 = 4/10$ proved the most fruitful.\relax }}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Method}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Resulting S-curve comparing the cumulative success of each type of agent to the episode in which the solution is found. The agents have a limit of 10,000 episodes or \SI {24}{\hour }. As can be seen on the figure, the $\epsilon $-loop agents all find the global solution significantly slower than the baseline. However, the $\epsilon $-loop agents end up with the same fraction of agents finding the global solution as the baseline. Since the uncertainties of the 2 types of agents overlap after episode 2500, we cannot claim that the performance of one agent is significantly different from the others after that episode.\relax }}{16}}
\newlabel{fig:S-curve}{{\M@TitleReference {6.2}{Resulting S-curve comparing the cumulative success of each type of agent to the episode in which the solution is found. The agents have a limit of 10,000 episodes or \SI {24}{\hour }. As can be seen on the figure, the $\epsilon $-loop agents all find the global solution significantly slower than the baseline. However, the $\epsilon $-loop agents end up with the same fraction of agents finding the global solution as the baseline. Since the uncertainties of the 2 types of agents overlap after episode 2500, we cannot claim that the performance of one agent is significantly different from the others after that episode.\relax }}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{16}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Discussion}{17}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {8}Conclusion}{18}}
\bibstyle{unsrt}
\bibdata{bibtex/Meldgaard.bib,bibtex/henrik_progress_report.bib,bibtex/Atomic_structure_learning.bib,bibtex/intro_sources}
\bibcite{Dumontier2017}{1}
\bibcite{Rojas96}{2}
\bibcite{WikiCNN}{3}
\bibcite{Wiki_RF}{4}
\bibcite{AlphaGo}{5}
\bibcite{Hammer}{6}
\bibcite{henrik_DLA}{7}
\bibcite{Meldgaard}{8}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{20}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Appendix - Extra figures}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {A1}{\ignorespaces This figure illustrates the possible problem that $\epsilon $-loop agents in ASLA may have encountered. The $\bm  {\varepsilon }$ has been fitted using 500 structures from episodes up to 1000. It has then predicted the energy of 500 structures from episode 1001 and onwards, chosen randomly. Note that DFT energies below $\approx \SI {-62}{\electronvolt }$ will be assigned that energy. The energy range of the structures that are used for training seems to determine the prediction range. This could be a problem, since the structure filter works by sorted structures after predicted energies.\relax }}{22}}
\newlabel{fig:App_low_energy_predicts}{{\M@TitleReference {A1}{This figure illustrates the possible problem that $\epsilon $-loop agents in ASLA may have encountered. The $\bm  {\varepsilon }$ has been fitted using 500 structures from episodes up to 1000. It has then predicted the energy of 500 structures from episode 1001 and onwards, chosen randomly. Note that DFT energies below $\approx \SI {-62}{\electronvolt }$ will be assigned that energy. The energy range of the structures that are used for training seems to determine the prediction range. This could be a problem, since the structure filter works by sorted structures after predicted energies.\relax }}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {A2}{\ignorespaces Enlarged version of figure \ref  {fig:filtering}.\relax }}{23}}
\newlabel{fig:App_Filtering}{{\M@TitleReference {A2}{Enlarged version of figure \ref  {fig:filtering}.\relax }}{23}}
\memsetcounter{lastsheet}{29}
\memsetcounter{lastpage}{23}
